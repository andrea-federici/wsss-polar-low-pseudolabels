{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5211,"status":"ok","timestamp":1728653244653,"user":{"displayName":"Andrea Federici","userId":"09647634370252844595"},"user_tz":-120},"id":"mlsaxiaoVfEf","outputId":"ca6b20aa-b544-424d-e161-91c91281d1e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.9)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.1+cu121)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.7)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (10.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/polar-lows-detection-forecasting-deep-learning\n"]}],"source":["if 'google.colab' in str(get_ipython()):\n","    !pip install timm\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    %cd /content/drive/MyDrive/polar-lows-detection-forecasting-deep-learning/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":557,"status":"ok","timestamp":1728653245574,"user":{"displayName":"Andrea Federici","userId":"09647634370252844595"},"user_tz":-120},"id":"kiu62oSnkKmx","outputId":"766fa826-9cf3-4378-8ffb-360bf994e8eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting models.py\n"]}],"source":["%%writefile models.py\n","\n","import torch\n","import torch.nn as nn\n","import timm\n","\n","class XceptionModel(nn.Module):\n","    def __init__(self, num_classes=2, freeze_features=True, fine_tune_layers=0):\n","        super(XceptionModel, self).__init__()\n","\n","        # Feature Extractor\n","        self.feature_extractor = timm.create_model('xception', pretrained=True, features_only=True)\n","\n","        # Inspect feature_info to determine the number of channels. For Xception this is typically 2048.\n","        feature_info = self.feature_extractor.feature_info\n","        num_channels = feature_info[-1]['num_chs']\n","        print(f'Number of channels in the last feature map: {num_channels}')\n","\n","        # Freeze feature extractor if specified\n","        if freeze_features:\n","            for param in self.feature_extractor.parameters():\n","                param.requires_grad = False\n","            print(\"Feature extractor frozen.\")\n","        else:\n","            print(\"Feature extractor not frozen.\")\n","\n","        # Fine-tune the last 'fine_tune_layers'\n","        if fine_tune_layers > 0:\n","            child_modules = list(self.feature_extractor.children())\n","            for module in modules[-fine_tune_layers:]:\n","                for param in module.parameters():\n","                    param.requires_grad = True\n","            print(f\"Enabled fine-tuning for the last {fine_tune_layers} layers.\")\n","\n","        # Classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(in_features=2048, out_features=32),\n","            nn.BatchNorm1d(32),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=0.2),\n","\n","            nn.Linear(in_features=32, out_features=num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        last_features = x[-1] # Get the last feature map (self.feature_extractor(x) is a list)\n","        pooled_features = last_features.mean(dim=[2, 3]) # Global Average Pooling. Input shape: (batch_size, channels, H, W). This layer reduces each feature map by averaging over the spatial dimensions (H, W), resulting in a tensor of shape (batch_size, channels).\n","        x = self.classifier(pooled_features)\n","        return x\n","\n","    def get_last_conv_layer(self):\n","        last_conv = None\n","        for name, layer in reversed(list(self.feature_extractor.named_modules())):\n","            if isinstance(layer, nn.Conv2d) and layer.kernel_size == (1,1): # We are interested in the last pointwise convolution layer.\n","                last_conv = layer\n","                print(f'Selected pointwise convolution layer: {name} - {layer}')\n","                break\n","        if last_conv is None:\n","            raise ValueError(\"No convolutional layer found in the feature extractor.\")\n","        return last_conv\n","\n","class ConvModel(nn.Module):\n","    def __init__(self, input_channels=3, num_classes=2):\n","        super(ConvModel, self).__init__()\n","\n","        # Feature Extractor\n","        self.feature_extractor = nn.Sequential(\n","\n","            # Block 1\n","            nn.Conv2d(in_channels=input_channels, out_channels=64, kernel_size=5, stride=2, padding=2),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, stride=2, padding=2),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Block 2\n","            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Block 3\n","            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Block 4\n","            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        )\n","\n","        # Global Average Pooling (GAP)\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # This layer reduces the spatial dimensions of the feature maps to 1x1\n","\n","        # Classifier\n","        self.classifier = nn.Linear(in_features=256, out_features=num_classes)\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        x = self.global_avg_pool(x)\n","        x = x.view(x.size(0), -1) # Flatten layer\n","        x = self.classifier(x)\n","        return x\n","\n","    def get_last_conv_layer(self):\n","        for layer in reversed(self.feature_extractor):\n","            if isinstance(layer, nn.Conv2d):\n","                return layer\n","        raise ValueError(\"No convolutional layer found in the feature extractor.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hr90Vs1YDdAe","executionInfo":{"status":"ok","timestamp":1728653136806,"user_tz":-120,"elapsed":5461,"user":{"displayName":"Andrea Federici","userId":"09647634370252844595"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8549e50e-9cf2-4de7-d14a-7bf2ce11ea57"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of channels in the last feature map: 2048\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 32, 255, 255]             864\n","       BatchNorm2d-2         [-1, 32, 255, 255]              64\n","              ReLU-3         [-1, 32, 255, 255]               0\n","            Conv2d-4         [-1, 64, 253, 253]          18,432\n","       BatchNorm2d-5         [-1, 64, 253, 253]             128\n","              ReLU-6         [-1, 64, 253, 253]               0\n","            Conv2d-7         [-1, 64, 253, 253]             576\n","            Conv2d-8        [-1, 128, 253, 253]           8,192\n","   SeparableConv2d-9        [-1, 128, 253, 253]               0\n","      BatchNorm2d-10        [-1, 128, 253, 253]             256\n","             ReLU-11        [-1, 128, 253, 253]               0\n","           Conv2d-12        [-1, 128, 253, 253]           1,152\n","           Conv2d-13        [-1, 128, 253, 253]          16,384\n","  SeparableConv2d-14        [-1, 128, 253, 253]               0\n","      BatchNorm2d-15        [-1, 128, 253, 253]             256\n","        MaxPool2d-16        [-1, 128, 127, 127]               0\n","           Conv2d-17        [-1, 128, 127, 127]           8,192\n","      BatchNorm2d-18        [-1, 128, 127, 127]             256\n","            Block-19        [-1, 128, 127, 127]               0\n","             ReLU-20        [-1, 128, 127, 127]               0\n","           Conv2d-21        [-1, 128, 127, 127]           1,152\n","           Conv2d-22        [-1, 256, 127, 127]          32,768\n","  SeparableConv2d-23        [-1, 256, 127, 127]               0\n","      BatchNorm2d-24        [-1, 256, 127, 127]             512\n","             ReLU-25        [-1, 256, 127, 127]               0\n","           Conv2d-26        [-1, 256, 127, 127]           2,304\n","           Conv2d-27        [-1, 256, 127, 127]          65,536\n","  SeparableConv2d-28        [-1, 256, 127, 127]               0\n","      BatchNorm2d-29        [-1, 256, 127, 127]             512\n","        MaxPool2d-30          [-1, 256, 64, 64]               0\n","           Conv2d-31          [-1, 256, 64, 64]          32,768\n","      BatchNorm2d-32          [-1, 256, 64, 64]             512\n","            Block-33          [-1, 256, 64, 64]               0\n","             ReLU-34          [-1, 256, 64, 64]               0\n","           Conv2d-35          [-1, 256, 64, 64]           2,304\n","           Conv2d-36          [-1, 728, 64, 64]         186,368\n","  SeparableConv2d-37          [-1, 728, 64, 64]               0\n","      BatchNorm2d-38          [-1, 728, 64, 64]           1,456\n","             ReLU-39          [-1, 728, 64, 64]               0\n","           Conv2d-40          [-1, 728, 64, 64]           6,552\n","           Conv2d-41          [-1, 728, 64, 64]         529,984\n","  SeparableConv2d-42          [-1, 728, 64, 64]               0\n","      BatchNorm2d-43          [-1, 728, 64, 64]           1,456\n","        MaxPool2d-44          [-1, 728, 32, 32]               0\n","           Conv2d-45          [-1, 728, 32, 32]         186,368\n","      BatchNorm2d-46          [-1, 728, 32, 32]           1,456\n","            Block-47          [-1, 728, 32, 32]               0\n","             ReLU-48          [-1, 728, 32, 32]               0\n","           Conv2d-49          [-1, 728, 32, 32]           6,552\n","           Conv2d-50          [-1, 728, 32, 32]         529,984\n","  SeparableConv2d-51          [-1, 728, 32, 32]               0\n","      BatchNorm2d-52          [-1, 728, 32, 32]           1,456\n","             ReLU-53          [-1, 728, 32, 32]               0\n","           Conv2d-54          [-1, 728, 32, 32]           6,552\n","           Conv2d-55          [-1, 728, 32, 32]         529,984\n","  SeparableConv2d-56          [-1, 728, 32, 32]               0\n","      BatchNorm2d-57          [-1, 728, 32, 32]           1,456\n","             ReLU-58          [-1, 728, 32, 32]               0\n","           Conv2d-59          [-1, 728, 32, 32]           6,552\n","           Conv2d-60          [-1, 728, 32, 32]         529,984\n","  SeparableConv2d-61          [-1, 728, 32, 32]               0\n","      BatchNorm2d-62          [-1, 728, 32, 32]           1,456\n","            Block-63          [-1, 728, 32, 32]               0\n","             ReLU-64          [-1, 728, 32, 32]               0\n","           Conv2d-65          [-1, 728, 32, 32]           6,552\n","           Conv2d-66          [-1, 728, 32, 32]         529,984\n","  SeparableConv2d-67          [-1, 728, 32, 32]               0\n","      BatchNorm2d-68          [-1, 728, 32, 32]           1,456\n","             ReLU-69          [-1, 728, 32, 32]               0\n","           Conv2d-70          [-1, 728, 32, 32]           6,552\n","           Conv2d-71          [-1, 728, 32, 32]         529,984\n","  SeparableConv2d-72          [-1, 728, 32, 32]               0\n","      BatchNorm2d-73          [-1, 728, 32, 32]           1,456\n","             ReLU-74          [-1, 728, 32, 32]               0\n","           Conv2d-75          [-1, 728, 32, 32]           6,552\n","           Conv2d-76          [-1, 728, 32, 32]         529,984\n","  SeparableConv2d-77          [-1, 728, 32, 32]               0\n","      BatchNorm2d-78          [-1, 728, 32, 32]           1,456\n","            Block-79          [-1, 728, 32, 32]               0\n","             ReLU-80          [-1, 728, 32, 32]               0\n","           Conv2d-81          [-1, 728, 32, 32]           6,552\n","           Conv2d-82          [-1, 728, 32, 32]         529,984\n","  SeparableConv2d-83          [-1, 728, 32, 32]               0\n","      BatchNorm2d-84          [-1, 728, 32, 32]           1,456\n","             ReLU-85          [-1, 728, 32, 32]               0\n","           Conv2d-86          [-1, 728, 32, 32]           6,552\n","           Conv2d-87          [-1, 728, 32, 32]         529,984\n","  SeparableConv2d-88          [-1, 728, 32, 32]               0\n","      BatchNorm2d-89          [-1, 728, 32, 32]           1,456\n","             ReLU-90          [-1, 728, 32, 32]               0\n","           Conv2d-91          [-1, 728, 32, 32]           6,552\n","           Conv2d-92          [-1, 728, 32, 32]         529,984\n","  SeparableConv2d-93          [-1, 728, 32, 32]               0\n","      BatchNorm2d-94          [-1, 728, 32, 32]           1,456\n","            Block-95          [-1, 728, 32, 32]               0\n","             ReLU-96          [-1, 728, 32, 32]               0\n","           Conv2d-97          [-1, 728, 32, 32]           6,552\n","           Conv2d-98          [-1, 728, 32, 32]         529,984\n","  SeparableConv2d-99          [-1, 728, 32, 32]               0\n","     BatchNorm2d-100          [-1, 728, 32, 32]           1,456\n","            ReLU-101          [-1, 728, 32, 32]               0\n","          Conv2d-102          [-1, 728, 32, 32]           6,552\n","          Conv2d-103          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-104          [-1, 728, 32, 32]               0\n","     BatchNorm2d-105          [-1, 728, 32, 32]           1,456\n","            ReLU-106          [-1, 728, 32, 32]               0\n","          Conv2d-107          [-1, 728, 32, 32]           6,552\n","          Conv2d-108          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-109          [-1, 728, 32, 32]               0\n","     BatchNorm2d-110          [-1, 728, 32, 32]           1,456\n","           Block-111          [-1, 728, 32, 32]               0\n","            ReLU-112          [-1, 728, 32, 32]               0\n","          Conv2d-113          [-1, 728, 32, 32]           6,552\n","          Conv2d-114          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-115          [-1, 728, 32, 32]               0\n","     BatchNorm2d-116          [-1, 728, 32, 32]           1,456\n","            ReLU-117          [-1, 728, 32, 32]               0\n","          Conv2d-118          [-1, 728, 32, 32]           6,552\n","          Conv2d-119          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-120          [-1, 728, 32, 32]               0\n","     BatchNorm2d-121          [-1, 728, 32, 32]           1,456\n","            ReLU-122          [-1, 728, 32, 32]               0\n","          Conv2d-123          [-1, 728, 32, 32]           6,552\n","          Conv2d-124          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-125          [-1, 728, 32, 32]               0\n","     BatchNorm2d-126          [-1, 728, 32, 32]           1,456\n","           Block-127          [-1, 728, 32, 32]               0\n","            ReLU-128          [-1, 728, 32, 32]               0\n","          Conv2d-129          [-1, 728, 32, 32]           6,552\n","          Conv2d-130          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-131          [-1, 728, 32, 32]               0\n","     BatchNorm2d-132          [-1, 728, 32, 32]           1,456\n","            ReLU-133          [-1, 728, 32, 32]               0\n","          Conv2d-134          [-1, 728, 32, 32]           6,552\n","          Conv2d-135          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-136          [-1, 728, 32, 32]               0\n","     BatchNorm2d-137          [-1, 728, 32, 32]           1,456\n","            ReLU-138          [-1, 728, 32, 32]               0\n","          Conv2d-139          [-1, 728, 32, 32]           6,552\n","          Conv2d-140          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-141          [-1, 728, 32, 32]               0\n","     BatchNorm2d-142          [-1, 728, 32, 32]           1,456\n","           Block-143          [-1, 728, 32, 32]               0\n","            ReLU-144          [-1, 728, 32, 32]               0\n","          Conv2d-145          [-1, 728, 32, 32]           6,552\n","          Conv2d-146          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-147          [-1, 728, 32, 32]               0\n","     BatchNorm2d-148          [-1, 728, 32, 32]           1,456\n","            ReLU-149          [-1, 728, 32, 32]               0\n","          Conv2d-150          [-1, 728, 32, 32]           6,552\n","          Conv2d-151          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-152          [-1, 728, 32, 32]               0\n","     BatchNorm2d-153          [-1, 728, 32, 32]           1,456\n","            ReLU-154          [-1, 728, 32, 32]               0\n","          Conv2d-155          [-1, 728, 32, 32]           6,552\n","          Conv2d-156          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-157          [-1, 728, 32, 32]               0\n","     BatchNorm2d-158          [-1, 728, 32, 32]           1,456\n","           Block-159          [-1, 728, 32, 32]               0\n","            ReLU-160          [-1, 728, 32, 32]               0\n","          Conv2d-161          [-1, 728, 32, 32]           6,552\n","          Conv2d-162          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-163          [-1, 728, 32, 32]               0\n","     BatchNorm2d-164          [-1, 728, 32, 32]           1,456\n","            ReLU-165          [-1, 728, 32, 32]               0\n","          Conv2d-166          [-1, 728, 32, 32]           6,552\n","          Conv2d-167          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-168          [-1, 728, 32, 32]               0\n","     BatchNorm2d-169          [-1, 728, 32, 32]           1,456\n","            ReLU-170          [-1, 728, 32, 32]               0\n","          Conv2d-171          [-1, 728, 32, 32]           6,552\n","          Conv2d-172          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-173          [-1, 728, 32, 32]               0\n","     BatchNorm2d-174          [-1, 728, 32, 32]           1,456\n","           Block-175          [-1, 728, 32, 32]               0\n","            ReLU-176          [-1, 728, 32, 32]               0\n","          Conv2d-177          [-1, 728, 32, 32]           6,552\n","          Conv2d-178          [-1, 728, 32, 32]         529,984\n"," SeparableConv2d-179          [-1, 728, 32, 32]               0\n","     BatchNorm2d-180          [-1, 728, 32, 32]           1,456\n","            ReLU-181          [-1, 728, 32, 32]               0\n","          Conv2d-182          [-1, 728, 32, 32]           6,552\n","          Conv2d-183         [-1, 1024, 32, 32]         745,472\n"," SeparableConv2d-184         [-1, 1024, 32, 32]               0\n","     BatchNorm2d-185         [-1, 1024, 32, 32]           2,048\n","       MaxPool2d-186         [-1, 1024, 16, 16]               0\n","          Conv2d-187         [-1, 1024, 16, 16]         745,472\n","     BatchNorm2d-188         [-1, 1024, 16, 16]           2,048\n","           Block-189         [-1, 1024, 16, 16]               0\n","          Conv2d-190         [-1, 1024, 16, 16]           9,216\n","          Conv2d-191         [-1, 1536, 16, 16]       1,572,864\n"," SeparableConv2d-192         [-1, 1536, 16, 16]               0\n","     BatchNorm2d-193         [-1, 1536, 16, 16]           3,072\n","            ReLU-194         [-1, 1536, 16, 16]               0\n","          Conv2d-195         [-1, 1536, 16, 16]          13,824\n","          Conv2d-196         [-1, 2048, 16, 16]       3,145,728\n"," SeparableConv2d-197         [-1, 2048, 16, 16]               0\n","     BatchNorm2d-198         [-1, 2048, 16, 16]           4,096\n","            ReLU-199         [-1, 2048, 16, 16]               0\n","AdaptiveAvgPool2d-200           [-1, 2048, 1, 1]               0\n","         Flatten-201                 [-1, 2048]               0\n","SelectAdaptivePool2d-202                 [-1, 2048]               0\n","        Identity-203                 [-1, 2048]               0\n","        Xception-204                 [-1, 2048]               0\n","  FeatureHookNet-205  [[-1, 64, 253, 253], [-1, 128, 127, 127], [-1, 256, 64, 64], [-1, 728, 32, 32], [-1, 2048, 16, 16]]               0\n","          Linear-206                   [-1, 32]          65,568\n","     BatchNorm1d-207                   [-1, 32]              64\n","            ReLU-208                   [-1, 32]               0\n","         Dropout-209                   [-1, 32]               0\n","          Linear-210                    [-1, 2]              66\n","================================================================\n","Total params: 20,872,650\n","Trainable params: 20,872,650\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 3.00\n","Forward/backward pass size (MB): 2104.88\n","Params size (MB): 79.62\n","Estimated Total Size (MB): 2187.50\n","----------------------------------------------------------------\n","Selected pointwise convolution layer: body.conv4.pointwise - Conv2d(1536, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n"]}],"source":["# from torchsummary import summary\n","\n","# model = XceptionModel()\n","\n","# summary(model, (3, 512, 512))\n","\n","# model.get_last_conv_layer();"]},{"cell_type":"code","source":[],"metadata":{"id":"lx15hMw93Kcb"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}