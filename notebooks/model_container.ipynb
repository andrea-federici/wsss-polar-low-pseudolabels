{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOQR/g0Fl2Lubic9JDyc7IU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["if 'google.colab' in str(get_ipython()):\n","    !pip install pytorch-lightning\n","    !pip install captum\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    %cd /content/drive/MyDrive/polar-lows-detection-forecasting-deep-learning/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P4NHMN5pt0Sz","executionInfo":{"status":"ok","timestamp":1729499135097,"user_tz":-120,"elapsed":64921,"user":{"displayName":"Andrea Federici","userId":"09647634370252844595"}},"outputId":"c487646a-6b0f-4277-da76-b643698fc6dd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-lightning\n","  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.4.1+cu121)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.5)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\n","Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n","  Downloading torchmetrics-1.5.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n","Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n","  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n","Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.15.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.0)\n","Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n","Downloading torchmetrics-1.5.0-py3-none-any.whl (890 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.5/890.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n","Successfully installed lightning-utilities-0.11.8 pytorch-lightning-2.4.0 torchmetrics-1.5.0\n","Collecting captum\n","  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.26.4)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.4.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from captum) (4.66.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.4.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2024.6.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (10.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (3.0.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n","Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: captum\n","Successfully installed captum-0.7.0\n","Mounted at /content/drive\n","/content/drive/MyDrive/polar-lows-detection-forecasting-deep-learning\n"]}]},{"cell_type":"code","source":["%%writefile model_container.py\n","\n","import cv2\n","\n","import numpy as np\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    precision_score,\n","    recall_score,\n","    f1_score\n",")\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","from torchvision.transforms import ToPILImage\n","import pytorch_lightning as pl\n","from captum.attr import (\n","    IntegratedGradients,\n","    LayerGradCam\n",")\n","\n","class ModelContainer(pl.LightningModule):\n","    def __init__(self, model: nn.Module, criterion: nn.Module, optimizer: optim.Optimizer = None):\n","        super(ModelContainer, self).__init__()\n","        self.model = model\n","        self.criterion = criterion # Loss function\n","        self.optimizer = optimizer # Defaults to Adam if not given as input\n","        self.train_losses = [] # Stores train losses for each epoch\n","        self.train_outputs = [] # Stores training outputs across steps\n","        self.val_losses = [] # Stores validation losses for each epoch\n","        self.val_outputs = [] # Stores validation outputs across steps\n","\n","    # The 'forward' method is called automatically when the model is invoked on input data\n","    def forward(self, x):\n","        return self.model(x) # self.model(x) calls the 'forward' method of the contained model\n","\n","    # Called for each batch of data\n","    def training_step(self, batch, batch_idx):\n","        images, labels = batch # Unpack the batch\n","        logits = self(images) # Forward pass to get predictions\n","        loss = self.criterion(logits, labels) # Calculate loss\n","\n","        preds = torch.argmax(logits, dim=1)\n","        self.train_outputs.append({\n","            'preds': preds.cpu().numpy(),\n","            'labels': labels.cpu().numpy(),\n","            'loss': loss.item()\n","        })\n","\n","        return loss # Return the loss for optimization\n","\n","    # Called at the end of every training epoch to aggregate metrics and print them\n","    def on_train_epoch_end(self):\n","        all_preds = [] # Collects all predictions from the epoch\n","        all_labels = [] # Collects all labels from the epoch\n","        total_loss = 0.0 # Tracks total loss across batches\n","\n","        for output in self.train_outputs:\n","            all_preds.extend(output['preds'])\n","            all_labels.extend(output['labels'])\n","            total_loss += output['loss']\n","\n","        avg_loss = total_loss / len(self.train_outputs) # Average loss for the epoch\n","        precision = precision_score(all_labels, all_preds, average='binary')\n","        recall = recall_score(all_labels, all_preds, average='binary')\n","        f1 = f1_score(all_labels, all_preds, average='binary')\n","\n","        self.train_losses.append(avg_loss) # Store the average loss for this epoch\n","\n","        print(f'Epoch {self.current_epoch} - Training:')\n","        print(f'Loss: {avg_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n","\n","        self.train_outputs.clear() # Clear the outputs for the next epoch\n","\n","    def validation_step(self, batch, batch_idx):\n","        images, labels = batch\n","        logits = self(images)\n","        loss = self.criterion(logits, labels) # Calculate validation loss\n","\n","        preds = torch.argmax(logits, dim=1)\n","\n","        # Store the step outputs for later use in the epoch end\n","        self.val_outputs.append({\n","            'preds': preds.cpu().numpy(),\n","            'labels': labels.cpu().numpy(),\n","            'loss': loss.item()\n","        })\n","\n","        self.log('val_loss', loss, prog_bar=True)\n","\n","    # At the end of each validation epoch, this method aggregates the outputs from all validation batches\n","    def on_validation_epoch_end(self):\n","        all_preds = []\n","        all_labels = []\n","        total_loss = 0.0\n","\n","        for output in self.val_outputs:\n","            all_preds.extend(output['preds'])\n","            all_labels.extend(output['labels'])\n","            total_loss += output['loss']\n","\n","        avg_loss = total_loss / len(self.val_outputs)\n","        precision = precision_score(all_labels, all_preds, average='binary')\n","        recall = recall_score(all_labels, all_preds, average='binary')\n","        f1 = f1_score(all_labels, all_preds, average='binary')\n","\n","        self.val_losses.append(avg_loss) # Store average validation loss for the epoch\n","\n","        print(f\"Epoch {self.current_epoch} - Validation:\")\n","        print(f\"Loss: {avg_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n","\n","        self.val_outputs.clear() # Clear validation outputs for the next epoch\n","\n","    def generate_gradcam_heatmap(self, input_image, layer=None):\n","        # If no layer is specified, use the last layer of the feature extractor\n","        if layer is None:\n","            layer = self.model.get_last_conv_layer()\n","\n","        gradcam = LayerGradCam(self.model, layer) # Initialize GradCAM method\n","\n","        self.eval()\n","\n","        input_image = input_image.to(self.device) # Move image to the same device as the model\n","\n","        logits = self(input_image)\n","        target_class = torch.argmax(logits, dim=1).item() # Get the predicted class\n","\n","        attr = gradcam.attribute(input_image, target_class) # Compute the GradCAM attributions\n","        attr = attr.squeeze().detach().cpu().numpy() # Convert to NumPy format\n","\n","        heatmap = np.maximum(attr, 0) # Remove negative values in the heatmap\n","        norm_heatmap = heatmap / np.max(heatmap) if np.max(heatmap) != 0 else 1 # Normalize the heatmap\n","\n","        return norm_heatmap\n","\n","    def generate_integrated_gradients_heatmap(self, input_image, target_class=None, n_steps=50, baseline=None):\n","        self.eval()\n","\n","        input_image = input_image.to(self.device) # Move image to the same device as the model\n","\n","        if baseline is None:\n","            baseline = torch.zeros_like(input_image) # Use a black image as the baseline is no baseline is provided\n","\n","        ig = IntegratedGradients(self.forward)\n","\n","        # Get predicted class\n","        logits = self(input_image)\n","        predicted_class = torch.argmax(logits, dim=1).item()\n","\n","        # If the target class is not specified, use the predicted class\n","        if target_class is None:\n","            target_class = predicted_class\n","\n","        attributions, _ = ig.attribute(\n","            input_image,\n","            baselines=baseline,\n","            target=target_class,\n","            n_steps=n_steps,\n","            return_convergence_delta=True\n","        )\n","\n","        attributions = attributions.squeeze().detach().cpu().numpy() # Convert to NumPy\n","\n","        # Aggregate attributions across channels\n","        attributions = np.mean(attributions, axis=0)\n","\n","        # Normalize the attributions to [0, 1]\n","        norm_attributions = (attributions - np.min(attributions)) / (np.max(attributions) - np.min(attributions) + 1e-8) # Normalize the attributions\n","\n","        # Enhance contrast\n","        # norm_attributions = cv2.equalizeHist((norm_attributions * 255).astype(np.uint8)) / 255.0\n","\n","        return norm_attributions, predicted_class\n","\n","    def overlay_green_heatmap(self, original_image, heatmap, predicted_class, target_class=None, alpha: float = 0.6, percentile_neg: float = 97, percentile_pos: float = 98, gaussian_blur_size: int = 15, verbose=False):\n","        image_np = np.array(original_image) # Convert the PIL image to a NumPy array\n","\n","        heatmap_resized = cv2.resize(heatmap, (image_np.shape[1], image_np.shape[0]))\n","\n","        # If the target class is not specified, use the predicted class\n","        if target_class is None:\n","            target_class = predicted_class\n","\n","        # Set the percentile based on the target class\n","        if target_class == 0:\n","            percentile = percentile_neg\n","        else:\n","            percentile = percentile_pos\n","\n","        # Flatten the heatmap\n","        heatmap_flattened = heatmap_resized.flatten()\n","\n","        # Calculate the threshold based on the percentile\n","        threshold = np.percentile(heatmap_flattened, percentile)\n","\n","        if verbose:\n","            print(f'Threshold for class {target_class} (percentile {percentile}): {threshold}')\n","\n","        # Apply threshold to focus on high attribution regions\n","        heatmap_thresholded = np.where(heatmap_resized >= threshold, heatmap_resized, 0)\n","\n","        # Create a green-only heatmap\n","        green_heatmap = np.zeros_like(image_np)\n","        green_heatmap[:, :, 1] = (heatmap_thresholded * 255).astype(np.uint8) # Green channel\n","\n","        # Apply a small Gaussian blur to make dots more visible\n","        green_heatmap = cv2.GaussianBlur(green_heatmap, (gaussian_blur_size, gaussian_blur_size), 0)\n","\n","        image_with_heatmap = cv2.addWeighted(green_heatmap, alpha, image_np, 1 - alpha, 0) # Overlay the heatmap on the original image\n","\n","        image_with_heatmap_pil = ToPILImage()(image_with_heatmap) # Convert the NumPy array back to PIL image\n","\n","        return image_with_heatmap_pil\n","\n","\n","    # Visualize the heatmap overlay on the original image\n","    def overlay_heatmap(self, original_image, heatmap, alpha: float = 0.4, colormap: int = cv2.COLORMAP_JET):\n","        image_np = np.array(original_image) # Convert the PIL image to a NumPy array\n","\n","        heatmap_resized = cv2.resize(heatmap, (image_np.shape[1], image_np.shape[0])) # Resize the heatmap to match the original image size\n","\n","        heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), colormap) # Apply color map to heatmap\n","        heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB) # Convert color space from BGR (Blue, Green, Red) to RGB\n","\n","        image_with_heatmap = cv2.addWeighted(heatmap_colored, alpha, image_np, 1 - alpha, 0) # Overlay the heatmap on the original image\n","\n","        image_with_heatmap_pil = ToPILImage()(image_with_heatmap) # Convert the NumPy array back to PIL image\n","\n","        return image_with_heatmap_pil\n","\n","    def configure_optimizers(self):\n","        if self.optimizer is None:\n","            # If no optimizer is provided, use Adam as default\n","            self.optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=0.001)\n","        return self.optimizer\n"],"metadata":{"id":"PlVtvoGKFzxI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729504467603,"user_tz":-120,"elapsed":260,"user":{"displayName":"Andrea Federici","userId":"09647634370252844595"}},"outputId":"329677bf-c6ba-4d74-f4d5-0e00edeccd6f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting model_container.py\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_y9MOyTftyaZ","executionInfo":{"status":"ok","timestamp":1729499135362,"user_tz":-120,"elapsed":5,"user":{"displayName":"Andrea Federici","userId":"09647634370252844595"}}},"execution_count":2,"outputs":[]}]}